{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb42617-498e-409e-b763-f4669b51f980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (counts):\n",
      " [[100   0]\n",
      " [  0 100]]\n",
      "\n",
      "Confusion matrix (normalized):\n",
      " [[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "TP=100, FP=0, FN=0, TN=100\n",
      "Accuracy=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000       100\n",
      "           1      1.000     1.000     1.000       100\n",
      "\n",
      "    accuracy                          1.000       200\n",
      "   macro avg      1.000     1.000     1.000       200\n",
      "weighted avg      1.000     1.000     1.000       200\n",
      "\n",
      "Saved test artifacts to: test_artifacts\n"
     ]
    }
   ],
   "source": [
    "# evaluate_on_test.py\n",
    "# This script loads the trained neural network model and evaluates it on a test dataset.\n",
    "# It computes the confusion matrix (counts and normalized) and common performance metrics.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from joblib import load\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    ")\n",
    "\n",
    "# Paths\n",
    "TEST_CSV = \"2halfmoonsTest.csv\"        # Path to the test dataset CSV file\n",
    "MODEL    = \"double_moon_model.joblib\"  # Trained model saved from part 1\n",
    "SCALER   = \"scaler.joblib\"             # StandardScaler saved from training\n",
    "OUTDIR   = \"test_artifacts\"            # Folder to save evaluation outputs\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# 1) Load test data (supports both 'X,Y,ClassLabel' or 'X,Y,label' column names)\n",
    "df = pd.read_csv(TEST_CSV)\n",
    "cols = {c.lower(): c for c in df.columns}\n",
    "\n",
    "if {\"x\", \"y\", \"classlabel\"}.issubset(cols):\n",
    "    X = df[[cols[\"x\"], cols[\"y\"]]].to_numpy(float)\n",
    "    y = df[cols[\"classlabel\"]].astype(int).to_numpy()\n",
    "elif {\"x\", \"y\", \"label\"}.issubset(cols):\n",
    "    X = df[[cols[\"x\"], cols[\"y\"]]].to_numpy(float)\n",
    "    y = df[cols[\"label\"]].astype(int).to_numpy()\n",
    "else:\n",
    "    # Default: assume first 2 columns are features, last one is label\n",
    "    X = df.iloc[:, :2].to_numpy(float)\n",
    "    y = df.iloc[:, -1].astype(int).to_numpy()\n",
    "\n",
    "# Normalize labels to {0,1} if they are {1,2}\n",
    "y = y - y.min()\n",
    "\n",
    "# 2) Load trained model and scaler\n",
    "mlp = load(MODEL)\n",
    "scaler = load(SCALER)\n",
    "\n",
    "# 3) Make predictions and compute metrics\n",
    "Xs = scaler.transform(X)\n",
    "proba = mlp.predict_proba(Xs)[:, 1]\n",
    "y_pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "cm_norm = confusion_matrix(y, y_pred, normalize=\"true\")\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y, y_pred)\n",
    "prec = precision_score(y, y_pred, zero_division=0)\n",
    "rec = recall_score(y, y_pred, zero_division=0)\n",
    "f1 = f1_score(y, y_pred, zero_division=0)\n",
    "\n",
    "# Print results\n",
    "print(\"Confusion matrix (counts):\\n\", cm)\n",
    "print(\"\\nConfusion matrix (normalized):\\n\", np.round(cm_norm, 3))\n",
    "print(f\"\\nTP={tp}, FP={fp}, FN={fn}, TN={tn}\")\n",
    "print(f\"Accuracy={acc:.4f}, Precision={prec:.4f}, Recall={rec:.4f}, F1={f1:.4f}\")\n",
    "print(\"\\nClassification report:\\n\", classification_report(y, y_pred, digits=3, zero_division=0))\n",
    "\n",
    "# 4) Save results to files for report\n",
    "np.savetxt(os.path.join(OUTDIR, \"confusion_matrix_counts.csv\"), cm, fmt=\"%d\", delimiter=\",\")\n",
    "np.savetxt(os.path.join(OUTDIR, \"confusion_matrix_normalized.csv\"), cm_norm, fmt=\"%.6f\", delimiter=\",\")\n",
    "with open(os.path.join(OUTDIR, \"test_metrics.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"TP\": int(tp), \"FP\": int(fp), \"FN\": int(fn), \"TN\": int(tn),\n",
    "        \"accuracy\": float(acc), \"precision\": float(prec),\n",
    "        \"recall\": float(rec), \"f1\": float(f1)\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Saved test artifacts to:\", OUTDIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ffe400-223a-46ce-904f-84a29b9b28d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f604ec-bd32-4481-a302-b7433bff4223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
